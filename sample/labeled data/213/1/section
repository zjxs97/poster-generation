the automatic post - editing task is to automatically correct errors in machine translation outputs . this paper describes our submission to the english - german ape shared task at wmt 2019 . based on recent research on the ape task and an architecture for the utilization of document - level context information in neural machine translation , we re - implement a multi - source transformer model for the task . inspired by cheng et al . , we try to train a more robust model by introducing a multi - task learning approach which jointly trains ape with a de - noising encoder .
we made use of the artificial escape data set provided for the task , since the multi - source transformer model contains a large number of parameters and training with large amounts of supplementary synthetic data can help regularize its parameters and make the model more general . we then tested the bleu scores between machine translation results and corresponding gold standard post - editing results on the original development set , the training set and the synthetic data as shown in table 1 .
table 1 shows that there is a significant gap be - tween the synthetic escape data set and the real - life data sets , potentially because negri et al . generated the data set in a different way compared to junczys - dowmunt and grundkiewicz and very few post - editing actions are normally required due to the good translation quality of neural machine translation which significantly reduces errors in machine translation results and makes the post - editing results quite similar to raw machine translation outputs .
